# -*- coding: utf-8 -*-
"""sliding_window_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylFrnx2Rd0u8gOkEsdi9K1okD_S27G8a
"""

import whisper

# load model (base or small for speed)
whisper_model = whisper.load_model("base")

from transformers import PegasusTokenizer, PegasusForConditionalGeneration, AutoTokenizer,AutoModelForSeq2SeqLM
import torch

from transformers import pipeline

pipe = pipeline("summarization", model="google/pegasus-arxiv")
tokenizer = AutoTokenizer.from_pretrained("google/pegasus-arxiv")
model = AutoModelForSeq2SeqLM.from_pretrained("google/pegasus-arxiv")

AUDIO_PATH = "lecture.mp3"   
CHUNK_WORDS = 400
device = "cuda" if torch.cuda.is_available() else "cpu"

whisper_model = whisper.load_model("base")

def transcribe_audio(AUDIO_PATH):
    print("Transcribing audio with correct spacing...")
    result = whisper_model.transcribe(AUDIO_PATH, verbose=False, condition_on_previous_text=False)

    tokenizer = whisper.tokenizer.get_tokenizer(multilingual=True)
    segments = []

    for seg in result["segments"]:
        tokens = seg.get("tokens", [])
        if tokens:
            clean_text = tokenizer.decode(tokens).strip()
        else:
            clean_text = " ".join(seg["text"].replace("\n", " ").split())
        segments.append({
            "start": seg["start"],
            "end": seg["end"],
            "text": clean_text
        })

    result["text"] = " ".join([s["text"] for s in segments])
    return segments


    for seg in segments:
        seg["text"] = seg["text"].strip()


    full_text = " ".join(seg["text"] for seg in segments)
    result["text"] = " ".join(full_text.split())  

    return segments


def chunk_segments(segments, chunk_size=400):
    chunks = []
    current_chunk = {"start": segments[0]["start"], "text": ""}
    word_count = 0

    for seg in segments:
        text = seg["text"].strip()
        words = text.split()
        word_count += len(words)
        current_chunk["text"] = (current_chunk["text"] + " " + text).strip()

        
        if word_count >= chunk_size:
            current_chunk["end"] = seg["end"]
            chunks.append(current_chunk)
            current_chunk = {"start": seg["start"], "text": ""}
            word_count = 0


    if current_chunk["text"].strip():
        current_chunk["end"] = segments[-1]["end"]
        chunks.append(current_chunk)

    return chunks

def summarize_text(text, min_len=20, max_len=30):
    inputs = tokenizer(
        text, truncation=True, padding="longest", return_tensors="pt", max_length=1024
    ).to(device)
    summary_ids = model.generate(
        **inputs,
        num_beams=6,
        no_repeat_ngram_size=3,
        min_length=min_len,
        max_length=max_len
    )
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def process_audio(AUDIO_PATH):
  segments = transcribe_audio(AUDIO_PATH)
  chunks = chunk_segments(segments)

  summaries = []

  for chunk in chunks:
    summary = summarize_text(chunk["text"])
    summaries.append(summary)

  return summaries